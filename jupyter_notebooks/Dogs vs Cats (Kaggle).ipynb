{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cats vs Dogs\n",
    "\n",
    "The purpose of this is to train a convnet from scratch using a small dataset. The data is from the Kaggle website and was a competition ran around September 2013.\n",
    "\n",
    "## Extract the images from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process skipped as content exists.\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "import keras\n",
    "\n",
    "original_dataset_dir = '/Users/datascience4/Downloads/kaggle_cats_and_dogs/train'\n",
    "\n",
    "#new directory for a smaller dataset than what is packed in kaggle dataset.\n",
    "base_dir = '/Users/datascience4/Documents/cats_and_dogs_small'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "## training set for the training validation and test splits\n",
    "## this will be broken down as:\n",
    "## training set 1000 samples (images) of each class (2000 total)\n",
    "## validation set of 500 samples (images) of each class (1000 total)\n",
    "## test set of 500 samples (images) of each class (1000 total)\n",
    "\n",
    "try:\n",
    "    os.mkdir(base_dir)\n",
    "    #make the train test and validation directories.\n",
    "    os.mkdir(train_dir)\n",
    "    os.mkdir(test_dir)\n",
    "    os.mkdir(validation_dir)\n",
    "    os.mkdir(train_cats_dir)\n",
    "    os.mkdir(train_dogs_dir)\n",
    "    os.mkdir(test_cats_dir)\n",
    "    os.mkdir(test_dogs_dir)\n",
    "    os.mkdir(validation_cats_dir)\n",
    "    os.mkdir(validation_dogs_dir)\n",
    "    \n",
    "except FileExistsError:\n",
    "    print('Process skipped as directories exists.')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the files in the folder and put them into train, validation, and test the respective files\n",
    "## using list comprehensions in a fast way to match the format on the files\n",
    "## f is short hand new in 3.6 python pep498 formatted string literals\n",
    "\n",
    "# training cats directory\n",
    "fnames = [f'cat.{i}.jpg' for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dest = os.path.join(train_cats_dir, fname)\n",
    "    shutil.copyfile(src, dest)\n",
    "\n",
    "# test cats directory\n",
    "fnames = [f'cat.{i}.jpg' for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dest = os.path.join(test_cats_dir, fname)\n",
    "    shutil.copyfile(src, dest)\n",
    "\n",
    "# validation cats directory\n",
    "fnames = [f'cat.{i}.jpg' for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dest = os.path.join(validation_cats_dir, fname)\n",
    "    shutil.copyfile(src, dest)\n",
    "\n",
    "#training dogs directory\n",
    "fnames = [f'dog.{i}.jpg' for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dest = os.path.join(train_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dest)\n",
    "\n",
    "#test dogs directory\n",
    "fnames = [f'dog.{i}.jpg' for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dest = os.path.join(test_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dest)\n",
    "    \n",
    "#validation dogs directory\n",
    "fnames = [f'dog.{i}.jpg' for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dest = os.path.join(validation_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training cat images 1000\n",
      "total test cat images 500\n",
      "total validation cat images 500\n",
      "total training dog images 1000\n",
      "total test dog images 500\n",
      "total validation dog images 500\n"
     ]
    }
   ],
   "source": [
    "print('total training cat images', len(os.listdir(train_cats_dir)))\n",
    "print('total test cat images', len(os.listdir(test_cats_dir)))\n",
    "print('total validation cat images', len(os.listdir(validation_cats_dir)))\n",
    "print('total training dog images', len(os.listdir(train_dogs_dir)))\n",
    "print('total test dog images', len(os.listdir(test_dogs_dir)))\n",
    "print('total validation dog images', len(os.listdir(validation_dogs_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the convnet on a binary classification problem.\n",
    "\n",
    "Because these images are bigger an the a more complex problem then MNIST digits, an additional Conv2D and Maxpooling will need to be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "         input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "## from the second layer, input shape is assumed from the first layer\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if this goes into a different environment other than a notebook\n",
    "## it is typical to import earlier than this.\n",
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.sequential.Sequential object at 0x1245c69e8>\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "?model.compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "The next thing to do is data preprocessing, since images are currently in jpeg they need to be reformatted into preprocessed floating point tensors before being fed into the network.\n",
    "\n",
    "* Read the picture file\n",
    "* Decode the jpeg content to RGB grids of pixels.\n",
    "* Convert these into floating-point tensors.\n",
    "* Rescale the pixel values (between 0 and 255) to the \\[0, 1\\] interval (neural networks prefer to deal with small input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# ?ImageDataGenerator\n",
    "\n",
    "## rescale all images by 1/255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(150, 150), #Re size all images to 150 x 150\n",
    "        batch_size=20,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: (20,)\n"
     ]
    }
   ],
   "source": [
    "keras.__version__\n",
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 52s 520ms/step - loss: 0.6905 - acc: 0.5175 - val_loss: 0.6763 - val_acc: 0.6120\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 51s 515ms/step - loss: 0.6564 - acc: 0.6125 - val_loss: 0.6719 - val_acc: 0.5640\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 51s 514ms/step - loss: 0.6133 - acc: 0.6600 - val_loss: 0.6389 - val_acc: 0.6350\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 52s 516ms/step - loss: 0.5649 - acc: 0.7070 - val_loss: 0.5855 - val_acc: 0.6930\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 52s 516ms/step - loss: 0.5341 - acc: 0.7330 - val_loss: 0.6650 - val_acc: 0.6080\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 52s 519ms/step - loss: 0.5139 - acc: 0.7475 - val_loss: 0.5804 - val_acc: 0.6880\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 52s 520ms/step - loss: 0.4910 - acc: 0.7555 - val_loss: 0.5592 - val_acc: 0.7260\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 53s 527ms/step - loss: 0.4541 - acc: 0.7800 - val_loss: 0.5856 - val_acc: 0.6840\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 53s 532ms/step - loss: 0.4346 - acc: 0.8000 - val_loss: 0.6321 - val_acc: 0.6690\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 55s 545ms/step - loss: 0.4104 - acc: 0.8110 - val_loss: 0.5562 - val_acc: 0.7250\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 54s 539ms/step - loss: 0.3878 - acc: 0.8290 - val_loss: 0.5620 - val_acc: 0.7430\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 53s 532ms/step - loss: 0.3610 - acc: 0.8410 - val_loss: 0.5866 - val_acc: 0.7130\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 55s 551ms/step - loss: 0.3423 - acc: 0.8515 - val_loss: 0.5425 - val_acc: 0.7360\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 54s 538ms/step - loss: 0.3115 - acc: 0.8745 - val_loss: 0.6773 - val_acc: 0.6950\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 54s 544ms/step - loss: 0.2986 - acc: 0.8765 - val_loss: 0.5597 - val_acc: 0.7450\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 55s 547ms/step - loss: 0.2796 - acc: 0.8810 - val_loss: 0.6106 - val_acc: 0.7330\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 56s 558ms/step - loss: 0.2495 - acc: 0.9050 - val_loss: 0.6389 - val_acc: 0.7270\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 55s 550ms/step - loss: 0.2291 - acc: 0.9085 - val_loss: 0.6410 - val_acc: 0.7400\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 54s 544ms/step - loss: 0.2124 - acc: 0.9210 - val_loss: 0.6338 - val_acc: 0.7410\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 55s 552ms/step - loss: 0.1865 - acc: 0.9335 - val_loss: 0.6441 - val_acc: 0.7300\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 54s 543ms/step - loss: 0.1729 - acc: 0.9335 - val_loss: 0.6824 - val_acc: 0.7330\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 55s 550ms/step - loss: 0.1472 - acc: 0.9445 - val_loss: 0.7087 - val_acc: 0.7440\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 56s 556ms/step - loss: 0.1375 - acc: 0.9560 - val_loss: 0.7419 - val_acc: 0.7340\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 55s 552ms/step - loss: 0.1133 - acc: 0.9650 - val_loss: 0.8104 - val_acc: 0.7410\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 56s 556ms/step - loss: 0.1021 - acc: 0.9670 - val_loss: 0.8153 - val_acc: 0.7300\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 54s 541ms/step - loss: 0.0886 - acc: 0.9750 - val_loss: 0.8064 - val_acc: 0.7470\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 56s 555ms/step - loss: 0.0769 - acc: 0.9790 - val_loss: 0.8306 - val_acc: 0.7500\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 55s 551ms/step - loss: 0.0727 - acc: 0.9760 - val_loss: 0.8676 - val_acc: 0.7430\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 56s 557ms/step - loss: 0.0607 - acc: 0.9845 - val_loss: 0.8518 - val_acc: 0.7440\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 56s 555ms/step - loss: 0.0483 - acc: 0.9845 - val_loss: 0.9294 - val_acc: 0.7390\n"
     ]
    }
   ],
   "source": [
    "## Fitting the model using a batch generator (because generators are\n",
    "## iterators, you need to define steps_per_epoch)\n",
    "## you can also pass in a validation_data argument which can be \n",
    "## either a data generator but it could also be a tuple of numpy arrays\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
