{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## example using the pretrained convnet\n",
    "from keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                 include_top=False,\n",
    "                 input_shape=(150, 150, 3))\n",
    "\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two approaches\n",
    "\n",
    "Running the convolutional base over your dataset will be fast, but won't allow data augmentation.\n",
    "\n",
    "Or\n",
    "\n",
    "Extending the model you have (conv_base) by adding Dense layers on top and running the whole thing end to end on the input data. This will allow data augmentation but is expensive.\n",
    "\n",
    "## Fast feature extraction without data augmentation\n",
    "Using the ImageDataGenerator to extract images as Numpy arrays as well as their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "base_dir = '/Users/datascience4/Documents/cats_and_dogs_small'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 20\n",
    "\n",
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
    "    labels = np.zeros(shape=(sample_count))\n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            # Note that since generators yield data indefinitely in a loop,\n",
    "            # we must `break` after every image has been seen once.\n",
    "            break\n",
    "    return features, labels\n",
    "\n",
    "train_features, train_labels = extract_features(train_dir, 2000)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 1000)\n",
    "test_features, test_labels = extract_features(test_dir, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4, 4, 512)\n"
     ]
    }
   ],
   "source": [
    "print(validation_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 8192)\n"
     ]
    }
   ],
   "source": [
    "## feed them into a dense layer of 8192\n",
    "train_features = np.reshape(train_features, (2000, 4 * 4 * 512))\n",
    "validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))\n",
    "test_features = np.reshape(test_features, (1000, 4 * 4 * 512))\n",
    "\n",
    "print(validation_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 2s 777us/step - loss: 0.5903 - acc: 0.6705 - val_loss: 0.4448 - val_acc: 0.8150\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s 584us/step - loss: 0.4361 - acc: 0.7960 - val_loss: 0.3724 - val_acc: 0.8400\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 1s 592us/step - loss: 0.3544 - acc: 0.8460 - val_loss: 0.3378 - val_acc: 0.8470\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s 606us/step - loss: 0.3184 - acc: 0.8665 - val_loss: 0.3155 - val_acc: 0.8650\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s 594us/step - loss: 0.2887 - acc: 0.8905 - val_loss: 0.3040 - val_acc: 0.8600\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s 573us/step - loss: 0.2591 - acc: 0.8935 - val_loss: 0.3007 - val_acc: 0.8740\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s 602us/step - loss: 0.2440 - acc: 0.9060 - val_loss: 0.2860 - val_acc: 0.8730\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s 575us/step - loss: 0.2291 - acc: 0.9105 - val_loss: 0.2813 - val_acc: 0.8720\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s 571us/step - loss: 0.2145 - acc: 0.9185 - val_loss: 0.2713 - val_acc: 0.8820\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s 572us/step - loss: 0.2122 - acc: 0.9160 - val_loss: 0.2695 - val_acc: 0.8760\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s 572us/step - loss: 0.1976 - acc: 0.9260 - val_loss: 0.2650 - val_acc: 0.8810\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s 577us/step - loss: 0.1813 - acc: 0.9360 - val_loss: 0.2638 - val_acc: 0.8810\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s 572us/step - loss: 0.1725 - acc: 0.9390 - val_loss: 0.2614 - val_acc: 0.8840\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s 572us/step - loss: 0.1710 - acc: 0.9360 - val_loss: 0.2620 - val_acc: 0.8850\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s 571us/step - loss: 0.1587 - acc: 0.9445 - val_loss: 0.2577 - val_acc: 0.8850\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s 571us/step - loss: 0.1483 - acc: 0.9480 - val_loss: 0.2603 - val_acc: 0.8860\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s 573us/step - loss: 0.1412 - acc: 0.9545 - val_loss: 0.2567 - val_acc: 0.8840\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s 567us/step - loss: 0.1337 - acc: 0.9545 - val_loss: 0.2558 - val_acc: 0.8830\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s 569us/step - loss: 0.1293 - acc: 0.9550 - val_loss: 0.2547 - val_acc: 0.8870\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s 569us/step - loss: 0.1318 - acc: 0.9535 - val_loss: 0.2562 - val_acc: 0.8820\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s 568us/step - loss: 0.1234 - acc: 0.9545 - val_loss: 0.2654 - val_acc: 0.8910\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s 569us/step - loss: 0.1169 - acc: 0.9635 - val_loss: 0.2565 - val_acc: 0.8900\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s 570us/step - loss: 0.1066 - acc: 0.9640 - val_loss: 0.2545 - val_acc: 0.8870\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s 573us/step - loss: 0.1044 - acc: 0.9660 - val_loss: 0.2574 - val_acc: 0.8920\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s 567us/step - loss: 0.1034 - acc: 0.9670 - val_loss: 0.2557 - val_acc: 0.8900\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s 570us/step - loss: 0.1019 - acc: 0.9675 - val_loss: 0.2570 - val_acc: 0.8890\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s 569us/step - loss: 0.0927 - acc: 0.9715 - val_loss: 0.2561 - val_acc: 0.8910\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s 576us/step - loss: 0.0914 - acc: 0.9720 - val_loss: 0.2583 - val_acc: 0.8890\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s 645us/step - loss: 0.0848 - acc: 0.9775 - val_loss: 0.2660 - val_acc: 0.8750\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s 599us/step - loss: 0.0843 - acc: 0.9775 - val_loss: 0.2574 - val_acc: 0.8890\n"
     ]
    }
   ],
   "source": [
    "## defining and training the densely connected classifier\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_features, train_labels,\n",
    "                   epochs=30,\n",
    "                   batch_size=20,\n",
    "                   validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bH', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy pretrained')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bH', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss pretrained')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still overfitting\n",
    "The technique above is overfitting from the beginning. Therefore data augmentation would be useful on a small dataset like this but it would now become computationally expensive. Therefore a GPU is recommended from this point onwards.\n",
    "\n",
    "You can the pretrained model to a Sequential model because they behave in a similar way to a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 16,812,353\n",
      "Trainable params: 16,812,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## adding a densely connected classifier on top of the convolutional base\n",
    "from keras import models, layers\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing the pretrained layer\n",
    "The pretrained dataset needs to be frozen to prevent the weights from being updated during training. Not doing may result in the dense layers on top (which are randomly initialised) updating weights which would then propagate through the network, virtually destorying the represenations previously learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the number of trainable weights before freezing the conv base:  30\n",
      "This is the number of trainable weights after freezing the conv base:  4\n"
     ]
    }
   ],
   "source": [
    "print('This is the number of trainable weights '\n",
    "     'before freezing the conv base: ', len(model.trainable_weights))\n",
    "conv_base.trainable = False\n",
    "print('This is the number of trainable weights '\n",
    "     'after freezing the conv base: ', len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four weight tensors are now trainable two per layer (the main weight matrix, and the bias vector).\n",
    "\n",
    "For the updates to take effect you must first compile the model. If you modify the weight trainability after compilation, you should then recompile the model or these changes will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 334s 3s/step - loss: 0.6073 - acc: 0.6850 - val_loss: 0.4701 - val_acc: 0.8100\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 319s 3s/step - loss: 0.4859 - acc: 0.7800 - val_loss: 0.3812 - val_acc: 0.8400\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 312s 3s/step - loss: 0.4401 - acc: 0.8005 - val_loss: 0.3494 - val_acc: 0.8560\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 313s 3s/step - loss: 0.3939 - acc: 0.8355 - val_loss: 0.3296 - val_acc: 0.8660\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 313s 3s/step - loss: 0.3836 - acc: 0.8210 - val_loss: 0.3283 - val_acc: 0.8470\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 313s 3s/step - loss: 0.3739 - acc: 0.8385 - val_loss: 0.2968 - val_acc: 0.8730\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 313s 3s/step - loss: 0.3604 - acc: 0.8350 - val_loss: 0.2979 - val_acc: 0.8770\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 313s 3s/step - loss: 0.3450 - acc: 0.8495 - val_loss: 0.2855 - val_acc: 0.8810\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 314s 3s/step - loss: 0.3415 - acc: 0.8470 - val_loss: 0.2855 - val_acc: 0.8820\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 314s 3s/step - loss: 0.3389 - acc: 0.8485 - val_loss: 0.2827 - val_acc: 0.8810\n",
      "Epoch 11/30\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.3333 - acc: 0.8626"
     ]
    }
   ],
   "source": [
    "## training the model end to end with a frozzen convultional base\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) #don't augment validation\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='binary')\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "             metrics=['acc'])\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
