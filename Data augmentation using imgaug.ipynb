{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def gen_matrix(image, nb_channels, random_state):\n",
    "      matrix_A = np.array([[0, -1, 0],\n",
    "                           [-1, 4, -1],\n",
    "                           [0, -1, 0]])\n",
    "      matrix_B = np.array([[0, 0, 0],\n",
    "                          [0, -4, 1],\n",
    "                          [0, 2, 1]])\n",
    "      if random_state.rand() < 0.5:\n",
    "          return [matrix_A] * nb_channels\n",
    "      else:\n",
    "          return [matrix_B] * nb_channels\n",
    "# aug = iaa.Convolve(matrix=gen_matrix)\n",
    "\n",
    "seq = iaa.Sequential([\n",
    "    ## unsure if Crop is likely to be useful offsett may be better\n",
    "#     iaa.Crop(px=(0, 16)), # crop images from each side by 0 to 16px (randomly chosen)\n",
    "    ## definitely worth doing Fliplr as it doubles the size.\n",
    "#     iaa.Fliplr(0.5), # horizontally flip 50% of the images\n",
    "    ## GaussianBlur is also worth performing on for generalising\n",
    "    ## possibly ignore it for my own data?  \n",
    "#     iaa.GaussianBlur(sigma=(0, 3.0)), # blur images with a sigma of 0 to 3.0\n",
    "    ## Noise is also worth doing for generalising.\n",
    "#     iaa.AdditiveGaussianNoise(scale=0.1*255) #looks like static\n",
    "#     iaa.EdgeDetect(alpha=(0.0, 1.0))\n",
    "    \n",
    "    ## I think convolve is probably useful if you define it with the sharpness to demonstrate\n",
    "    ## the advantages of using Convolve is to view what the Computer might see.\n",
    "    iaa.Convolve(matrix=gen_matrix),\n",
    "    ## sharpeness definitely feels useful for defining the edges of the area.\n",
    "#     iaa.Sharpen(alpha=(0.0, 1.0), lightness=(0.75, 2.0))\n",
    "    ## probably worth doing sometimes, it might be better with \n",
    "#     iaa.ContrastNormalization((0.5, 1.5))\n",
    "#     iaa.ContrastNormalization((0.5, 1.5), per_channel=0.5),\n",
    "    ## Affine scale\n",
    "    ## might be an issue due to where the signs are typically located don't increase the sign beyond 1.0\n",
    "    ## unless you write a function to discard bounding boxes outside the actual area.\n",
    "#     iaa.Affine(scale=(0.35, .90)),\n",
    "    ## scale indepently on each axis indepently changing axis, might be really useful\n",
    "#     iaa.Affine(scale={'x': (0.5, 1), 'y': (0.35, 1.0)}),\n",
    "    ## this could be useful however, it would require it the translation to not cut off the signs in the images\n",
    "    ## possible function to ensure it's not missing\n",
    "    ## note though it's on percent\n",
    "#     iaa.Affine(translate_percent={'x': (-0.02, 0.2), 'y': (-0.2, 0.2)})\n",
    "    ## note the below is a pixel translation which might actually be better due to it being more minor\n",
    "#     iaa.Affine(translate_pixel={'x': (-20, 20), 'y': (-20, 20)})\n",
    "    ## note that this will be worth while if it's using with scaling and/or doesn't cut off\n",
    "    ## the actual sign\n",
    "#     iaa.Affine(rotate=(-45, 45))\n",
    "    ## shear is definitely worth including in the data augmentation process\n",
    "#     iaa.Affine(shear=(-160,160))\n",
    "    ## unsure of what the mode kwarg is doing currently\n",
    "#     iaa.Affine(translate_percent={\"x\": (-0.20)}, mode=ia.ALL, cval=(0, 255))\n",
    "    ## Really useful on low level distortion\n",
    "#     iaa.PiecewiseAffine(scale=(0.008, 0.0095))\n",
    "    ## use ElasticTransformation to add random can pass in tuples for randomness\n",
    "    ## to the distortion field\n",
    "    iaa.ElasticTransformation(alpha=(0, 5.0), sigma=(0.25, 1))\n",
    "\n",
    "])\n",
    "img1 = cv2.imread('/Users/datascience4/Documents/highways-deep-learning/data/A3400_NB1_RAV_R07_180516084515_frame_235.jpg')\n",
    "\n",
    "img = cv2.imread('/Users/datascience4/Documents/highways-deep-learning/data/A3400_NB1_RAV_R07_180516084515_frame_105.jpg')\n",
    "# cv2.imshow('test', img)\n",
    "imgs = [img, img1]\n",
    "print(type(img))\n",
    "# seq.show_grid(imgs[0], cols=8, rows=8)\n",
    "# cropped = img[7:100, 22:67]\n",
    "# # cv2.imshow('cropped', cropped)\n",
    "# # cv2.waitKey(0)\n",
    "\n",
    "img_aug = seq.augment_image(img)\n",
    "# seq.show_grid(img_aug, cols=8, rows=8)\n",
    "# print(img.shape)\n",
    "# print(img_aug.shape)\n",
    "# print(len(img_aug))\n",
    "\n",
    "# cv2.imshow('aug??', img_aug)\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "# # for batch_idx in range(1000):\n",
    "# #     # 'images' should be either a 4D numpy array of shape (N, height, width, channels)\n",
    "# #     # or a list of 3D numpy arrays, each having shape (height, width, channels).\n",
    "# #     # Grayscale images must have shape (height, width, 1) each.\n",
    "# #     # All images must have numpy's dtype uint8. Values are expected to be in\n",
    "# #     # range 0-255.\n",
    "# #     images = load_batch(batch_idx)\n",
    "# #     images_aug = seq.augment_images(images)\n",
    "# #     train_on_images(images_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write out one image as new.\n",
    "\n",
    "1. It needs the path of the image.\n",
    "2. The bounding boxes of the image.\n",
    "3. Write the number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385\n",
      "['A3400_NB1_RAV_R07_180516084515_frame_3598.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2862.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2447.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2335.xml', 'A3400_NB1_RAV_R07_180516084515_frame_4022.xml', 'A3400_NB1_RAV_R07_180516084515_frame_224.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2255.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2527.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3163.xml', 'A3400_NB1_RAV_R07_180516084515_frame_350.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3375.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3374.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3837.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3162.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2240.xml', 'A3400_NB1_RAV_R07_180516084515_frame_225.xml', 'A3400_NB1_RAV_R07_180516084515_frame_4023.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2446.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2863.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3599.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2678.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2108.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2861.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2450.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2336.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3000.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3148.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2256.xml', 'A3400_NB1_RAV_R07_180516084515_frame_4141.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3160.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1978.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3376.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3377.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3161.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2257.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3149.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2445.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2451.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2337.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2870.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2864.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2496.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3950.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3165.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3603.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3159.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3830.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1216.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3373.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3429.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3398.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3372.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3831.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3825.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3158.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1029.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3164.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3602.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2440.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2865.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2871.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1411.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2867.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3589.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3984.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1188.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3600.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3166.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1981.xml', 'A3400_NB1_RAV_R07_180516084515_frame_369.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3827.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3370.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1215.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1598.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1214.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3371.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3826.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3832.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1980.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3629.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3601.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3985.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3588.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2866.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1412.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3538.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2183.xml', 'A3400_NB_RAV_R07_180514164328_frame_2143.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2424.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2430.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3706.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2418.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3855.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3841.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2020.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1529.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2746.xml', 'A3400_NB1_RAV_R07_180516084515_frame_119.xml', 'A3400_NB1_RAV_R07_180516084515_frame_118.xml', 'A3400_NB1_RAV_R07_180516084515_frame_642.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3317.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2021.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2747.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3840.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3854.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3115.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2237.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2419.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3707.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2431.xml', 'A3400_NB_RAV_R07_180514164328_frame_2142.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2425.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2182.xml', 'A3400_NB1_RAV_R07_180516084515_frame_911.xml', 'A3400_NB1_RAV_R07_180516084515_frame_24.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3539.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2433.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2427.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3705.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3063.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3117.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3842.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3856.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2745.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2023.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3498.xml', 'A3400_NB1_RAV_R07_180516084515_frame_640.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3499.xml', 'A3400_NB1_RAV_R07_180516084515_frame_899.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2022.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3857.xml', 'A3400_NB1_RAV_R07_180516084515_frame_443.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3843.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3116.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3704.xml', 'A3400_NB_RAV_R07_180514164328_frame_1460.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2426.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2432.xml', 'A3400_NB_RAV_R07_180514164328_frame_1879.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2181.xml', 'A3400_NB1_RAV_R07_180516084515_frame_912.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1463.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3502.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3258.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2185.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3066.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2422.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1856.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2387.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3890.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3847.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1936.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3853.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2026.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2998.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2032.xml', 'A3400_NB1_RAV_R07_180516084515_frame_122.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2033.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2027.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2999.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3852.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3846.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1937.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2386.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1857.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2423.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3067.xml', 'A3400_NB_RAV_R07_180514164328_frame_1303.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2184.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3259.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3503.xml', 'A3400_NB1_RAV_R07_180516084515_frame_22.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1466.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3501.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1464.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3529.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2186.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3703.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3065.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1855.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2384.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3139.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1909.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3850.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3844.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2019.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2031.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2025.xml', 'A3400_NB1_RAV_R07_180516084515_frame_120.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1289.xml', 'A3400_NB1_RAV_R07_180516084515_frame_121.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1288.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2024.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2030.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2018.xml', 'A3400_NB1_RAV_R07_180516084515_frame_445.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3845.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3851.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1908.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3138.xml', 'A3400_NB1_RAV_R07_180516084515_frame_4119.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2385.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1854.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3064.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2187.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3528.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1465.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3500.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3257.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3531.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3069.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2439.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2388.xml', 'A3400_NB1_RAV_R07_180516084515_frame_4114.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1911.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1939.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3848.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1093.xml', 'A3400_NB1_RAV_R07_180516084515_frame_4316.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2997.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2029.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3323.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1534.xml', 'A3400_NB1_RAV_R07_180516084515_frame_105.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3322.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2996.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2028.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1938.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3849.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1910.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3120.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2389.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3068.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3256.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3530.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3532.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3254.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1133.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3917.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1912.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1906.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3320.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1286.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1287.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2995.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3321.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2017.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1907.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1913.xml', 'A3400_NB_RAV_R07_180514164328_frame_1247.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3137.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3533.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3255.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3537.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2826.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3709.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2417.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2239.xml', 'A3400_NB1_RAV_R07_180516084515_frame_472.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3325.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1532.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3319.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2013.xml', 'A3400_NB1_RAV_R07_180516084515_frame_117.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2012.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3318.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1533.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3324.xml', 'A3400_NB1_RAV_R07_180516084515_frame_4113.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2562.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2238.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2416.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3708.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2827.xml', 'A3400_NB1_RAV_R07_180516084515_frame_4265.xml', 'A3400_NB_RAV_R07_180514164328_frame_1646.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3536.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3534.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2825.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2428.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3118.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2560.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3859.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1914.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3326.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1531.xml', 'A3400_NB1_RAV_R07_180516084515_frame_897.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3497.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3496.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1530.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1915.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3858.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3119.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2561.xml', 'A3400_NB_RAV_R07_180514164328_frame_1255.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2415.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2429.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3535.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3585.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3591.xml', 'A3400_NB1_RAV_R07_180516084515_frame_991.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2472.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3156.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3432.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3397.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3396.xml', 'A3400_NB_RAV_R07_180514164328_frame_1025.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3433.xml', 'A3400_NB1_RAV_R07_180516084515_frame_4374.xml', 'A3400_NB1_RAV_R07_180516084515_frame_370.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3157.xml', 'A3400_NB1_RAV_R07_180516084515_frame_204.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2473.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3590.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3584.xml', 'A3400_NB1_RAV_R07_180516084515_frame_953.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2868.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3592.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3586.xml', 'A3400_NB1_RAV_R07_180516084515_frame_4028.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1187.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3141.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3155.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3633.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3828.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3431.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1597.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1596.xml', 'A3400_NB_RAV_R07_180514164328_frame_1797.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3430.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3829.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3154.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3632.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3140.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1186.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3587.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3593.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2869.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1343.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3540.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2676.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3597.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3583.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2448.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2474.xml', 'A3400_NB1_RAV_R07_180516084515_frame_203.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2528.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3839.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3434.xml', 'A3400_NB1_RAV_R07_180516084515_frame_4415.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3435.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3838.xml', 'A3400_NB_RAV_R07_180514164328_frame_2072.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2529.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3151.xml', 'A3400_NB1_RAV_R07_180516084515_frame_202.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2475.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3596.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2677.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3541.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1342.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1340.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2107.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3594.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3153.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3147.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2259.xml', 'A3400_NB1_RAV_R07_180516084515_frame_348.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1977.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3437.xml', 'A3400_NB1_RAV_R07_180516084515_frame_4416.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3436.xml', 'A3400_NB1_RAV_R07_180516084515_frame_349.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2258.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3152.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2476.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3595.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2674.xml', 'A3400_NB1_RAV_R07_180516084515_frame_2106.xml', 'A3400_NB1_RAV_R07_180516084515_frame_1341.xml', 'A3400_NB1_RAV_R07_180516084515_frame_3542.xml']\n",
      "/Users/datascience4/Documents/highways-deep-learning/data/A3400_NB1_RAV_R07_180516084515_frame_3598.jpg\n",
      "737 204 793 250\n",
      "BB 0:, ((737, 204, 793, 250)           -> ((569.9175424710576, 294.8513465763965, 635.1215857856165, 357.84847563579024))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "\n",
    "base_dir = '/Users/datascience4/Documents/training3'\n",
    "print(len(os.listdir(base_dir)))\n",
    "xml_files = os.listdir(base_dir)\n",
    "xml_files = [x for x in os.listdir(base_dir) if '.xml' in x]\n",
    "print(xml_files)\n",
    "items = []\n",
    "\n",
    "## come back to this\n",
    "\n",
    "#let's read the first xml\n",
    "test_xml = os.path.join(base_dir, xml_files[0])\n",
    "tree = ET.parse(test_xml)\n",
    "root = tree.getroot()\n",
    "## path can be used for cv2 read image\n",
    "path = root.find('path').text\n",
    "print(path)\n",
    "## find the object and xmin, ymin, xmax, ymax\n",
    "for child in root.iter('object'):\n",
    "    if child.find('bndbox'):\n",
    "        bndbox = child.find('bndbox')\n",
    "        xmin = int(bndbox.find('xmin').text)\n",
    "        ymin = int(bndbox.find('ymin').text)\n",
    "        xmax = int(bndbox.find('xmax').text)\n",
    "        ymax = int(bndbox.find('ymax').text)\n",
    "        \n",
    "print(xmin, ymin, xmax, ymax)\n",
    "\n",
    "# ia.seed(1)\n",
    "## pass the path into cv2\n",
    "image = cv2.imread(path)\n",
    "# cv2.imshow('path', image)\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "## bounding boxes now for the image takes a list\n",
    "bbs = ia.BoundingBoxesOnImage([\n",
    "    ia.BoundingBox(x1=xmin, y1=ymin, x2=xmax, y2=ymax)\n",
    "], shape=image.shape)\n",
    "\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Multiply((1.2, 1.5)), # change brightness, doesn't affect bb\n",
    "    iaa.Affine(\n",
    "        translate_px={\"x\": 40, \"y\": 60},\n",
    "#         scale=(0.5, 0.7),\n",
    "        rotate=(-25, 25)\n",
    "        \n",
    "    ) # translate by 40/60px on x/y axis, and scale to 50-70%, affects BBs\n",
    "])\n",
    "\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5), #horizontal flips\n",
    "    iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "    #Small gaussian blur with random sigma between 0 and 0.5\n",
    "    # But we only blur about 50% of all images.\n",
    "    iaa.Sometimes(0.5,\n",
    "                 iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "    ),\n",
    "    #Strengthen or weaken the contrast in each image.\n",
    "    iaa.ContrastNormalization((0.75, 1.5)),\n",
    "    #Add gaussian noise.\n",
    "    #For 50% of all images, we sample the noise once per pixel..\n",
    "    #For the other 50% of all images, we sample the noise per pixel\n",
    "    #AND channel. This can change the color (not only the brightness)\n",
    "    #of the pixels.\n",
    "    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255),\n",
    "                             per_channel=0.5),\n",
    "    #Make some images brighter and some darker.\n",
    "    #in 20% of all cases we sample the multiplier once per channel.\n",
    "    # which can end up changing the color of the images.\n",
    "    iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "    #Apply affine transformations to each image.\n",
    "    #Scale/zoom them, translate/move them, rotate them, and shear them.\n",
    "    iaa.Affine(\n",
    "        scale={'x': (0.8, 1.2), 'y': (0.8, 1.2)},\n",
    "        translate_percent={'x': (-0.2, 0.2), 'y': (-0.2, 0.2)},\n",
    "        rotate=(-25, 25),\n",
    "        shear=(-8, 8)\n",
    "    )\n",
    "], random_order=True) #apply augmenters in random order\n",
    "\n",
    "## make our sequence deterministic.\n",
    "# we can now apply it to the image and then to the bbs and it will\n",
    "# lead to the same augmentations\n",
    "# IMPORTANT: call this once PER BATCH, otherwise you will always\n",
    "# get the exactly same augmentations for every batch!\n",
    "seq_det = seq.to_deterministic()\n",
    "\n",
    "##Augment bbs and images.\n",
    "## as we only have one image and list of bbs we use\n",
    "## [image] and [bbs] to turn both into lists (batches) for the\n",
    "## functions and then [0] to reverse that. In a real experiment, your\n",
    "## variables would likely already be lists.\n",
    "image_aug = seq_det.augment_images([image])[0]\n",
    "bbs_aug = seq_det.augment_bounding_boxes([bbs])[0]\n",
    "\n",
    "##print the coordinates before/after augmentation(see below)\n",
    "## use .x1_int, y1_int, ... to get interger coordinates\n",
    "for i in range(len(bbs.bounding_boxes)):\n",
    "    before = bbs.bounding_boxes[i]\n",
    "    after = bbs_aug.bounding_boxes[i]\n",
    "    print(f\"BB {i}:, ({before.x1, before.y1, before.x2, before.y2} \\\n",
    "          -> ({after.x1, after.y1, after.x2, after.y2})\")\n",
    "    \n",
    "# image with bbs before/after augmentation (shown below)\n",
    "image_before = bbs.draw_on_image(image, thickness=2)\n",
    "# cv2.imshow('image before', image_before)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "image_after = bbs_aug.draw_on_image(image_aug, thickness=2, color=[0, 0, 255])\n",
    "# cv2.imshow('image after', image_after)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "cv2.imwrite('/Users/datascience4/Desktop/test1.png', image_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to do\n",
    "1. Create a folder for the augmented images\n",
    "2. Then run through all images (drawing on each image the bounding box)\n",
    "3. Sense check that all are in the bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder already exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir('/Users/datascience4/Documents/training_3_data_aug')\n",
    "except FileExistsError:\n",
    "    print('folder already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "/Users/datascience4/Documents/highways-deep-learning/data/A3400_NB1_RAV_R07_180516084515_frame_3598.jpg\n",
      "BoundingBoxesOnImage([BoundingBox(x1=737.0000, y1=204.0000, x2=793.0000, y2=250.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=459.9096, y1=150.9540, x2=536.0286, y2=214.1741, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=737.0000, y1=204.0000, x2=793.0000, y2=250.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=723.9367, y1=273.7677, x2=805.9799, y2=346.7471, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=737.0000, y1=204.0000, x2=793.0000, y2=250.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=548.8507, y1=139.8402, x2=619.2448, y2=184.1468, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=737.0000, y1=204.0000, x2=793.0000, y2=250.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=708.6929, y1=382.2601, x2=787.9956, y2=432.1236, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=737.0000, y1=204.0000, x2=793.0000, y2=250.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=791.4690, y1=297.7795, x2=844.3706, y2=355.5882, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=737.0000, y1=204.0000, x2=793.0000, y2=250.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=993.6165, y1=226.5909, x2=1062.3816, y2=270.0763, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=737.0000, y1=204.0000, x2=793.0000, y2=250.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=517.0395, y1=93.9663, x2=599.2562, y2=168.7199, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=737.0000, y1=204.0000, x2=793.0000, y2=250.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=548.1994, y1=274.6752, x2=601.7294, y2=339.0034, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=737.0000, y1=204.0000, x2=793.0000, y2=250.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=670.0300, y1=225.8091, x2=761.0312, y2=299.8003, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=737.0000, y1=204.0000, x2=793.0000, y2=250.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=787.5061, y1=206.6838, x2=879.9272, y2=271.6287, label=None)], shape=(768, 1360, 3))\n",
      "1\n",
      "/Users/datascience4/Documents/highways-deep-learning/data/A3400_NB1_RAV_R07_180516084515_frame_2862.jpg\n",
      "BoundingBoxesOnImage([BoundingBox(x1=845.0000, y1=200.0000, x2=873.0000, y2=224.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=1255.3726, y1=143.0492, x2=1298.0294, y2=173.3932, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=845.0000, y1=200.0000, x2=873.0000, y2=224.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=885.0951, y1=158.1000, x2=916.8436, y2=189.5054, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=845.0000, y1=200.0000, x2=873.0000, y2=224.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=819.9967, y1=76.0631, x2=858.2398, y2=114.2143, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=845.0000, y1=200.0000, x2=873.0000, y2=224.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=688.3665, y1=10.1465, x2=730.4694, y2=42.6707, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=845.0000, y1=200.0000, x2=873.0000, y2=224.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=342.1976, y1=155.0583, x2=379.7023, y2=181.2418, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=845.0000, y1=200.0000, x2=873.0000, y2=224.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=1134.1587, y1=228.3967, x2=1172.1021, y2=262.7612, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=845.0000, y1=200.0000, x2=873.0000, y2=224.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=646.9508, y1=232.7880, x2=680.3199, y2=259.0441, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=845.0000, y1=200.0000, x2=873.0000, y2=224.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=1117.9774, y1=257.4515, x2=1156.6281, y2=289.2888, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=845.0000, y1=200.0000, x2=873.0000, y2=224.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=646.7638, y1=194.7758, x2=680.0474, y2=223.8341, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=845.0000, y1=200.0000, x2=873.0000, y2=224.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=977.5774, y1=72.0906, x2=1015.0234, y2=104.2473, label=None)], shape=(768, 1360, 3))\n",
      "2\n",
      "/Users/datascience4/Documents/highways-deep-learning/data/A3400_NB1_RAV_R07_180516084515_frame_2447.jpg\n",
      "partly?\n",
      "partly?\n",
      "BoundingBoxesOnImage([BoundingBox(x1=322.0000, y1=124.0000, x2=362.0000, y2=181.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=905.9364, y1=-24.2552, x2=953.3821, y2=47.5548, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=322.0000, y1=124.0000, x2=362.0000, y2=181.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=1100.1927, y1=348.3489, x2=1158.3039, y2=415.7920, label=None)], shape=(768, 1360, 3))\n",
      "partly?\n",
      "partly?\n",
      "BoundingBoxesOnImage([BoundingBox(x1=322.0000, y1=124.0000, x2=362.0000, y2=181.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=541.0509, y1=-42.3489, x2=583.8434, y2=32.6120, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=322.0000, y1=124.0000, x2=362.0000, y2=181.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=202.5703, y1=95.4415, x2=250.6787, y2=160.5824, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=322.0000, y1=124.0000, x2=362.0000, y2=181.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=433.5110, y1=137.6835, x2=490.7826, y2=192.9058, label=None)], shape=(768, 1360, 3))\n",
      "not in at all\n",
      "BoundingBoxesOnImage([BoundingBox(x1=322.0000, y1=124.0000, x2=362.0000, y2=181.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=572.1595, y1=-140.0271, x2=626.5404, y2=-59.3663, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=322.0000, y1=124.0000, x2=362.0000, y2=181.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=1217.8811, y1=243.6460, x2=1275.3048, y2=307.7268, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=322.0000, y1=124.0000, x2=362.0000, y2=181.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=331.1441, y1=45.2581, x2=384.1246, y2=114.5102, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=322.0000, y1=124.0000, x2=362.0000, y2=181.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=1036.6840, y1=113.7627, x2=1104.8809, y2=183.7538, label=None)], shape=(768, 1360, 3))\n",
      "not in at all\n",
      "BoundingBoxesOnImage([BoundingBox(x1=322.0000, y1=124.0000, x2=362.0000, y2=181.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=900.6583, y1=-121.3912, x2=943.5087, y2=-47.4983, label=None)], shape=(768, 1360, 3))\n",
      "3\n",
      "/Users/datascience4/Documents/highways-deep-learning/data/A3400_NB1_RAV_R07_180516084515_frame_2335.jpg\n",
      "BoundingBoxesOnImage([BoundingBox(x1=245.0000, y1=173.0000, x2=287.0000, y2=208.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=937.3977, y1=231.5475, x2=990.2453, y2=275.1324, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=245.0000, y1=173.0000, x2=287.0000, y2=208.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=356.4790, y1=113.8269, x2=422.3805, y2=160.8062, label=None)], shape=(768, 1360, 3))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoundingBoxesOnImage([BoundingBox(x1=245.0000, y1=173.0000, x2=287.0000, y2=208.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=817.6052, y1=373.2601, x2=868.2175, y2=420.1802, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=245.0000, y1=173.0000, x2=287.0000, y2=208.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=1142.7012, y1=444.4355, x2=1197.8214, y2=502.2867, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=245.0000, y1=173.0000, x2=287.0000, y2=208.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=437.1789, y1=85.5049, x2=493.5734, y2=136.7982, label=None)], shape=(768, 1360, 3))\n",
      "partly?\n",
      "partly?\n",
      "BoundingBoxesOnImage([BoundingBox(x1=245.0000, y1=173.0000, x2=287.0000, y2=208.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=337.6682, y1=-36.4746, x2=393.9716, y2=16.5791, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=245.0000, y1=173.0000, x2=287.0000, y2=208.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=881.5531, y1=40.4873, x2=937.2366, y2=80.3038, label=None)], shape=(768, 1360, 3))\n",
      "partly?\n",
      "partly?\n",
      "BoundingBoxesOnImage([BoundingBox(x1=245.0000, y1=173.0000, x2=287.0000, y2=208.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=-18.0734, y1=-27.4548, x2=29.5542, y2=22.7235, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=245.0000, y1=173.0000, x2=287.0000, y2=208.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=206.5369, y1=218.8875, x2=246.9220, y2=256.8848, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=245.0000, y1=173.0000, x2=287.0000, y2=208.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=75.9956, y1=15.6694, x2=133.9539, y2=65.8847, label=None)], shape=(768, 1360, 3))\n",
      "4\n",
      "/Users/datascience4/Documents/highways-deep-learning/data/A3400_NB1_RAV_R07_180516084515_frame_4022.jpg\n",
      "BoundingBoxesOnImage([BoundingBox(x1=898.0000, y1=188.0000, x2=1143.0000, y2=286.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=106.5948, y1=260.0137, x2=464.7063, y2=470.2926, label=None)], shape=(768, 1360, 3))\n",
      "partly?\n",
      "partly?\n",
      "BoundingBoxesOnImage([BoundingBox(x1=898.0000, y1=188.0000, x2=1143.0000, y2=286.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=644.7656, y1=-98.8943, x2=953.8172, y2=66.4961, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=898.0000, y1=188.0000, x2=1143.0000, y2=286.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=932.8493, y1=42.9071, x2=1225.9017, y2=189.4038, label=None)], shape=(768, 1360, 3))\n",
      "partly?\n",
      "partly?\n",
      "BoundingBoxesOnImage([BoundingBox(x1=898.0000, y1=188.0000, x2=1143.0000, y2=286.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=1206.2891, y1=179.6158, x2=1518.8070, y2=322.0006, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=898.0000, y1=188.0000, x2=1143.0000, y2=286.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=246.3667, y1=182.5465, x2=544.6111, y2=372.9252, label=None)], shape=(768, 1360, 3))\n",
      "partly?\n",
      "partly?\n",
      "BoundingBoxesOnImage([BoundingBox(x1=898.0000, y1=188.0000, x2=1143.0000, y2=286.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=113.1105, y1=-82.2803, x2=369.6469, y2=125.6398, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=898.0000, y1=188.0000, x2=1143.0000, y2=286.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=372.6034, y1=24.6318, x2=745.5260, y2=258.4752, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=898.0000, y1=188.0000, x2=1143.0000, y2=286.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=888.6827, y1=215.3349, x2=1124.6072, y2=332.4552, label=None)], shape=(768, 1360, 3))\n",
      "partly?\n",
      "partly?\n",
      "BoundingBoxesOnImage([BoundingBox(x1=898.0000, y1=188.0000, x2=1143.0000, y2=286.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=1196.4679, y1=328.1885, x2=1464.4122, y2=490.7586, label=None)], shape=(768, 1360, 3))\n",
      "BoundingBoxesOnImage([BoundingBox(x1=898.0000, y1=188.0000, x2=1143.0000, y2=286.0000, label=None)], shape=(768, 1360, 3)) BoundingBoxesOnImage([BoundingBox(x1=1000.4763, y1=211.2894, x2=1236.9441, y2=409.3525, label=None)], shape=(768, 1360, 3))\n"
     ]
    }
   ],
   "source": [
    "aug_dir = '/Users/datascience4/Documents/training_3_data_aug'\n",
    "\n",
    "## SECTION DEALS WITH BOUNDING BOXES NOT IN THE IMAGE\n",
    "## funtion to pad the image and deal with bounding boxes not in the images\n",
    "\n",
    "# Pad image with a 1px white and (BY-1)px black border\n",
    "def pad(image, by):\n",
    "    if by <= 0:\n",
    "        return image\n",
    "    image_border1 = np.pad(\n",
    "        image, ((1, 1), (1, 1), (0, 0)),\n",
    "        mode=\"constant\", constant_values=255\n",
    "    )\n",
    "    image_border2 = np.pad(\n",
    "        image_border1, ((by-1, by-1), (by-1, by-1), (0, 0)),\n",
    "        mode=\"constant\", constant_values=0\n",
    "    )\n",
    "    return image_border2\n",
    "\n",
    "# Draw BBs on an image\n",
    "# and before doing that, extend the image plane by BORDER pixels.\n",
    "# Mark BBs inside the image plane with green color, those partially inside\n",
    "# with orange and those fully outside with red.\n",
    "def draw_bbs(image, bbs, border):\n",
    "    ##rgb in open cv isn't rgb but bgr - it's bad for that!\n",
    "    GREEN = (0, 255, 0)\n",
    "    ORANGE = (0, 140, 255)\n",
    "    RED = (0, 0, 255)\n",
    "    image_border = pad(image, border)\n",
    "    for bb in bbs.bounding_boxes:\n",
    "        if bb.is_fully_within_image(image.shape):\n",
    "            color = GREEN\n",
    "        elif bb.is_partly_within_image(image.shape):\n",
    "            print('partly?')\n",
    "            color = ORANGE\n",
    "        else:\n",
    "            print('not in at all')\n",
    "            color = RED\n",
    "        image_border = bb.shift(left=border, top=border).draw_on_image(image_border, thickness=2, color=color)\n",
    "    return image_border\n",
    "\n",
    "\n",
    "\n",
    "## attempt to write out the same image multiple times in different ways\n",
    "for i in range(5):\n",
    "    #let's read the first xml\n",
    "    test_xml = os.path.join(base_dir, xml_files[i])\n",
    "    print(i)\n",
    "    tree = ET.parse(test_xml)\n",
    "    root = tree.getroot()\n",
    "    ## path can be used for cv2 read image\n",
    "    path = root.find('path').text\n",
    "    print(path)\n",
    "    ## find the object and xmin, ymin, xmax, ymax\n",
    "    for child in root.iter('object'):\n",
    "        if child.find('bndbox'):\n",
    "            bndbox = child.find('bndbox')\n",
    "            xmin = int(bndbox.find('xmin').text)\n",
    "            ymin = int(bndbox.find('ymin').text)\n",
    "            xmax = int(bndbox.find('xmax').text)\n",
    "            ymax = int(bndbox.find('ymax').text)\n",
    "    \n",
    "    image = cv2.imread(path)\n",
    "    bbs = ia.BoundingBoxesOnImage([\n",
    "                    ia.BoundingBox(x1=xmin, y1=ymin, x2=xmax, y2=ymax)\n",
    "    ], shape=image.shape)\n",
    "    for j in range(10):\n",
    "        seq_det = seq.to_deterministic()\n",
    "        image_aug = seq_det.augment_images([image])[0]\n",
    "        bbs_aug = seq_det.augment_bounding_boxes([bbs])[0]\n",
    "        image_after = bbs_aug.draw_on_image(image_aug, thickness=2, color=[255, 140, 0])\n",
    "        ## testing\n",
    "        image_after1 = draw_bbs(image_aug, bbs_aug, 100)\n",
    "        cv2.imwrite(os.path.join(aug_dir, ('bb' + str(i) + str(j) + '.png')), image_after1)\n",
    "        image_after2 = draw_bbs(image_aug, bbs_aug.remove_out_of_image(), 100)\n",
    "        cv2.imwrite(os.path.join(aug_dir, ('bb_removed' + str(i) + str(j) + '.png')), image_after2)\n",
    "        image_after3 = draw_bbs(image_aug, bbs_aug.remove_out_of_image()\\\n",
    "                .cut_out_of_image(), 100)\n",
    "        cv2.imwrite(os.path.join(aug_dir, ('bb_removed_and_cut_out' + str(i) + str(j) + '.png')), image_after3)\n",
    "#         cv2.imwrite(os.path.join(aug_dir, (str(i) + str(j) + '.png')), image_after)\n",
    "        print(bbs, bbs_aug)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The above works but needs improvement\n",
    "Things now required.\n",
    "* Bounding boxes need to be in the image, if they aren't they should be disgarded and a new image created that until the bounding box sits inside the image and can then be counted.\n",
    "* Understanding how to disgard the images that do not have a bounding box inside the image.\n",
    "* Need to now disgar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BoundingBox(x1=898.0000, y1=188.0000, x2=1143.0000, y2=286.0000, label=None)]\n",
      "BoundingBox(x1=898.0000, y1=188.0000, x2=1143.0000, y2=286.0000, label=None)\n"
     ]
    }
   ],
   "source": [
    "print(bbs.bounding_boxes)\n",
    "for bb in bbs.bounding_boxes:\n",
    "    print(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 1360, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
